{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages, Environment Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, get_embeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Use the PyPDF2 library to read a PDF file\n",
    "#from pypdf import PdfReader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sentence_transformers import CrossEncoder\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGINE = 'text-embedding-ada-002'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x27e1357ee50 state=finished raised AuthenticationError>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\embeddings_utils.py:23\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(text, engine, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mEmbedding\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m[text], engine\u001b[38;5;241m=\u001b[39mengine, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_resources\\embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# This is only for the default case.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:151\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m     (\n\u001b[0;32m    141\u001b[0m         deployment_id,\n\u001b[0;32m    142\u001b[0m         engine,\n\u001b[0;32m    143\u001b[0m         timeout,\n\u001b[0;32m    144\u001b[0m         stream,\n\u001b[0;32m    145\u001b[0m         headers,\n\u001b[0;32m    146\u001b[0m         request_timeout,\n\u001b[0;32m    147\u001b[0m         typed_api_type,\n\u001b[0;32m    148\u001b[0m         requestor,\n\u001b[0;32m    149\u001b[0m         url,\n\u001b[0;32m    150\u001b[0m         params,\n\u001b[1;32m--> 151\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__prepare_create_request(\n\u001b[0;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m     )\n\u001b[0;32m    155\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    157\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    163\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:108\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[1;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    106\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m MAX_TIMEOUT\n\u001b[1;32m--> 108\u001b[0m requestor \u001b[38;5;241m=\u001b[39m api_requestor\u001b[38;5;241m.\u001b[39mAPIRequestor(\n\u001b[0;32m    109\u001b[0m     api_key,\n\u001b[0;32m    110\u001b[0m     api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[0;32m    111\u001b[0m     api_type\u001b[38;5;241m=\u001b[39mapi_type,\n\u001b[0;32m    112\u001b[0m     api_version\u001b[38;5;241m=\u001b[39mapi_version,\n\u001b[0;32m    113\u001b[0m     organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[0;32m    114\u001b[0m )\n\u001b[0;32m    115\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mclass_url(engine, api_type, api_version)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_requestor.py:139\u001b[0m, in \u001b[0;36mAPIRequestor.__init__\u001b[1;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_base \u001b[38;5;241m=\u001b[39m api_base \u001b[38;5;129;01mor\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_base\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m key \u001b[38;5;129;01mor\u001b[39;00m util\u001b[38;5;241m.\u001b[39mdefault_api_key()\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_type \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    141\u001b[0m     ApiType\u001b[38;5;241m.\u001b[39mfrom_str(api_type)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m api_type\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ApiType\u001b[38;5;241m.\u001b[39mfrom_str(openai\u001b[38;5;241m.\u001b[39mapi_type)\n\u001b[0;32m    144\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\util.py:186\u001b[0m, in \u001b[0;36mdefault_api_key\u001b[1;34m()\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAuthenticationError(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo API key provided. You can set your API key in code using \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai.api_key = <API-KEY>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai.api_key_path = <PATH>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embedded_text \u001b[38;5;241m=\u001b[39m get_embedding(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI love to be vectorized\u001b[39m\u001b[38;5;124m'\u001b[39m, engine\u001b[38;5;241m=\u001b[39mENGINE)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:326\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n\u001b[0;32m    329\u001b[0m     sleep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(retry_state)\n",
      "\u001b[1;31mRetryError\u001b[0m: RetryError[<Future at 0x27e1357ee50 state=finished raised AuthenticationError>]"
     ]
    }
   ],
   "source": [
    "embedded_text = get_embedding('I love to be vectorized', engine=ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/41 [00:00<00:02, 16.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 27.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Open the PDF file in read-binary mode\n",
    "with open('../data/Introduction to Kubernetes.pdf', 'rb') as file:\n",
    "\n",
    "    # Create a PDF reader object\n",
    "    reader = PdfReader(file)\n",
    "\n",
    "    # Initialize an empty string to hold the text\n",
    "    intro_to_kube = ''\n",
    "\n",
    "    # Loop through each page in the PDF file\n",
    "    for page in tqdm(reader.pages):\n",
    "       \n",
    "        # Extract the text from the page\n",
    "        text = page.extract_text()\n",
    "\n",
    "        # Find the starting point of the text we want to extract\n",
    "        # In this case, we are extracting text starting from the string ' ]'\n",
    "        # intro_to_kube += '\\n\\n' + text[text.find(' ]')+2:]\n",
    "        intro_to_kube += '\\n\\n' + text\n",
    "\n",
    "# Strip any leading or trailing whitespace from the resulting string\n",
    "intro_to_kube = intro_to_kube.strip()\n",
    "print(len(intro_to_kube))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46150\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'(?<=\\S)\\n(?=\\S)'\n",
    "replacement = ' '\n",
    "intro_to_kube = re.sub(pattern, replacement, intro_to_kube)\n",
    "print(len(intro_to_kube))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36661, 1070]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the tiktoken library\n",
    "import tiktoken\n",
    "\n",
    "# Initializing a tokenizer for the 'cl100k_base' model\n",
    "# This tokenizer is designed to work with the 'ada-002' embedding model\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Using the tokenizer to encode the text 'hey there'\n",
    "# The resulting output is a list of integers representing the encoded text\n",
    "# This is the input format required for embedding using the 'ada-002' model\n",
    "tokenizer.encode('hey there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77, 3458, 336, 53413, 1618]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('nnaemeka here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Method (redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Function to split the text into chunks of a maximum number of tokens. Inspired by OpenAI\n",
    "def overlapping_chunks(text, max_tokens = 500, overlapping_factor = 5):\n",
    "    '''\n",
    "    max_tokens: tokens we want per chunk\n",
    "    overlapping_factor: number of sentences to start each chunk with that overlaps with the previous chunk\n",
    "    '''\n",
    "\n",
    "    # Split the text using punctuation\n",
    "    sentences = re.split(r'[.?!]', text)\n",
    "\n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks, tokens_so_far, chunk = [], 0, []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            if overlapping_factor > 0:\n",
    "                chunk = chunk[-overlapping_factor:]\n",
    "                tokens_so_far = sum([len(tokenizer.encode(c)) for c in chunk])\n",
    "            else:\n",
    "                chunk = []\n",
    "                tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of \n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "    if chunk:\n",
    "        chunks.append(\". \".join(chunk) + \".\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "#\n",
    "\n",
    "# A textbook about insects\n",
    "text = urlopen('https://www.gutenberg.org/cache/epub/10834/pg10834.txt').read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping chunking approach has 17 documents with average length 476.7 tokens\n"
     ]
    }
   ],
   "source": [
    "split = overlapping_chunks(text, overlapping_factor=0)\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\n",
    "print(f'non-overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlapping chunking approach has 24 documents with average length 477.4 tokens\n"
     ]
    }
   ],
   "source": [
    "split = overlapping_chunks(text)\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\n",
    "print(f'overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"semantic-search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"multi-qa-mpnet-base-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"/tmp/semantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    "    embedding_function=sentence_transformer_ef\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ae76cc4dfd345ecaeea9b8ba0d5c3437'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_hash(s):\n",
    "    # Return the MD5 hash of the input string as a hexadecimal string\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "my_hash('I love to hash it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_chroma(texts, engine=None):\n",
    "    now = datetime.utcnow()\n",
    "\n",
    "    if engine:\n",
    "        embeddings = get_embeddings(texts, engine=ENGINE)\n",
    "        return {\n",
    "        'ids':[my_hash(text) for text in texts],\n",
    "        'documents': [text for text in texts],\n",
    "        'embeddings': [embedding for embedding in embeddings],\n",
    "        'metadata': [dict(head=text[0], date_uploaded=str(now)) for text in texts]\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'ids':[my_hash(text) for text in texts],\n",
    "        'documents': [text for text in texts],\n",
    "        'metadata': [dict(head=text[0], date_uploaded=str(now)) for text in texts]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "response =  prepare_for_chroma(texts, engine=ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['49f68a5c8493ec2c0bf489821c21fc3b'],\n",
       " 'documents': ['hi'],\n",
       " 'metadata': [{'head': 'h', 'date_uploaded': '2023-10-20 13:24:57.880216'}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_texts_to_chroma(texts, collection, batch_size=None, show_progress_bar=True, engine=None):\n",
    "    total_added = 0\n",
    "    if not batch_size:\n",
    "        batch_size = len(texts)\n",
    "\n",
    "    _range = range(0, len(texts), batch_size)\n",
    "    for i in tqdm(_range) if show_progress_bar else _range:\n",
    "        batch = texts[i : i + batch_size]\n",
    "        output = prepare_for_chroma(batch, engine=engine)\n",
    "\n",
    "        if output.get('embeddings', None):\n",
    "            out = collection.add(\n",
    "                documents= output['documents'],\n",
    "                embeddings= output['embeddings'],\n",
    "                metadatas= output['metadata'],\n",
    "                ids= output['ids']\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            out = collection.add(\n",
    "                documents= output['documents'],\n",
    "                metadatas= output['metadata'],\n",
    "                ids= output['ids']\n",
    "                )\n",
    "        print(out)\n",
    "        total_added += 1\n",
    "\n",
    "        return total_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_texts_to_chroma(texts, collection, engine=ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_from_chroma(query, collection, engine=None, top_k=3):\n",
    "    if engine:\n",
    "        query_embedding = get_embedding(query, engine=ENGINE)\n",
    "\n",
    "        return collection.query(\n",
    "            query_embeddings=query_embedding,\n",
    "            n_results=top_k\n",
    "            )\n",
    "    return collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=top_k\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': [['49f68a5c8493ec2c0bf489821c21fc3b']],\n",
       " 'distances': [[0.07519697162713412]],\n",
       " 'metadatas': [[{'date_uploaded': '2023-10-18 09:35:59.505288'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['hi']]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_from_chroma('hello', collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_texts_from_chroma(texts, collection):\n",
    "    hashes = [my_hash(text) for text in texts]\n",
    "\n",
    "    return collection.delete(\n",
    "        ids=hashes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete text\n",
    "delete_texts_from_chroma(texts, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test collection is empty\n",
    "query_from_chroma('hello', collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Custom Delimiters\n",
    "\n",
    "#### This method requires hands-on familiarity of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 7093), ('\\n\\n\\n', 29), ('\\n\\n', 11)]\n"
     ]
    }
   ],
   "source": [
    "# Importing the Counter and re libraries\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Find all occurrences of one or more spaces in 'principles_of_ds'\n",
    "matches = re.findall(r'[\\s]{1,}', intro_to_kube)\n",
    "\n",
    "# The 10 most frequent spaces that occur in the document\n",
    "most_common_spaces = Counter(matches).most_common(10)\n",
    "\n",
    "# Print the most common spaces and their frequencies\n",
    "print(most_common_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep documents of at least 50 characters split by a custom delimiter\n",
    "split = list(filter(lambda x: len(x) > 50, intro_to_kube.split('\\n\\n\\n')))\n",
    "\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\n",
    "print(f'custom delimiter approach has {len(split)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process the documents in batches to obtain embeddings for each document\n",
    "embeddings = None\n",
    "for s in tqdm(range(0, len(split), 100)):\n",
    "    if embeddings is None:\n",
    "        embeddings = np.array(get_embeddings(split[s:s+100], engine=ENGINE))\n",
    "    else:\n",
    "        embeddings = np.vstack([embeddings, np.array(get_embeddings(split[s:s+100], engine=ENGINE))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: 2 embeddings\n",
      "Cluster 1: 2 embeddings\n",
      "Cluster 2: 2 embeddings\n",
      "Cluster 3: 1 embeddings\n",
      "Cluster 4: 1 embeddings\n",
      "Cluster 5: 1 embeddings\n",
      "Cluster 6: 1 embeddings\n",
      "Cluster 7: 1 embeddings\n",
      "Cluster 8: 1 embeddings\n",
      "Cluster 9: 1 embeddings\n",
      "Cluster 10: 1 embeddings\n",
      "Cluster 11: 1 embeddings\n",
      "Cluster 12: 1 embeddings\n",
      "Cluster 13: 1 embeddings\n",
      "Cluster 14: 1 embeddings\n",
      "Cluster 15: 1 embeddings\n",
      "Cluster 16: 1 embeddings\n",
      "Cluster 17: 1 embeddings\n",
      "Cluster 18: 1 embeddings\n",
      "Cluster 19: 1 embeddings\n",
      "Cluster 20: 1 embeddings\n",
      "Cluster 21: 1 embeddings\n",
      "Cluster 22: 1 embeddings\n",
      "Cluster 23: 1 embeddings\n",
      "Cluster 24: 1 embeddings\n",
      "Cluster 25: 1 embeddings\n",
      "Cluster 26: 1 embeddings\n",
      "Cluster 27: 1 embeddings\n",
      "Cluster 28: 1 embeddings\n",
      "Cluster 29: 1 embeddings\n",
      "Cluster 30: 1 embeddings\n",
      "Cluster 31: 1 embeddings\n",
      "Cluster 32: 1 embeddings\n",
      "Cluster 33: 1 embeddings\n",
      "Cluster 34: 1 embeddings\n",
      "Cluster 35: 1 embeddings\n",
      "Cluster 36: 1 embeddings\n",
      "Cluster 37: 1 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Assume you have a list of text embeddings called `embeddings`\n",
    "# First, compute the cosine similarity matrix between all pairs of embeddings\n",
    "cosine_sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Instantiate the AgglomerativeClustering model\n",
    "agg_clustering = AgglomerativeClustering(\n",
    "    n_clusters=None,         # the algorithm will determine the optimal number of clusters based on the data\n",
    "    distance_threshold=0.1,  # clusters will be formed until all pairwise distances between clusters are greater than 0.1\n",
    "    metric='precomputed',  # we are providing a precomputed distance matrix (1 - similarity matrix) as input\n",
    "    linkage='complete'       # form clusters by iteratively merging the smallest clusters based on the maximum distance between their components\n",
    ")\n",
    "\n",
    "# Fit the model to the cosine distance matrix (1 - similarity matrix)\n",
    "agg_clustering.fit(1 - cosine_sim_matrix)\n",
    "\n",
    "# Get the cluster labels for each embedding\n",
    "cluster_labels = agg_clustering.labels_\n",
    "\n",
    "# Print the number of embeddings in each cluster\n",
    "unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f'Cluster {label}: {count} embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our pruning approach has 38 documents with average length 238.2 tokens\n"
     ]
    }
   ],
   "source": [
    "pruned_documents = []\n",
    "for _label, count in zip(unique_labels, counts):\n",
    "    pruned_documents.append('\\n\\n'.join([text for text, label in zip(split, cluster_labels) if label == _label]))\n",
    "\n",
    "    \n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in pruned_documents]) / len(pruned_documents)\n",
    "print(f'Our pruning approach has {len(pruned_documents)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minikube Installation Binary download or Debian Package Mac OS 1. Verify virtualization for MacOS. VMX in the output indicates enabled virtualization 2. Install the VirtualBox hypervisor for MacOs 3. Download and install the .dmg package.\n",
      "\n",
      "\n",
      "Minikube Installation Install using brew package manager Or Binary Download Windows Installation 1. Verify the virtualization support on your Windows system (multiple output lines ending with 'Yes' indicate supported virtualization)\n"
     ]
    }
   ],
   "source": [
    "print(pruned_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Vector Database with OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_texts_to_chroma(pruned_documents, collection, batch_size=128, engine=ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I setup Kubernetes?\"\n",
    "\n",
    "results_from_chroma = query_from_chroma(query, collection, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['19551b58f60e0ded85d29c635ca4e5f1',\n",
       "   'e6f6650f9b61b98de63bd87f092d153d',\n",
       "   'b46e395aaf05e8f1e8e74bd574a01f97',\n",
       "   '2ac07e37162a9b0a64cdf6b4a5ae9927',\n",
       "   'fc8a457e7b543faad654c3d50ba94b24']],\n",
       " 'distances': [[0.14350228015213295,\n",
       "   0.15193963205075378,\n",
       "   0.152135480726164,\n",
       "   0.15850031407387533,\n",
       "   0.16811634666797715]],\n",
       " 'metadatas': [[{'date_uploaded': '2023-10-18 11:35:06.903148', 'head': 'R'},\n",
       "   {'date_uploaded': '2023-10-18 11:35:06.903148', 'head': 'C'},\n",
       "   {'date_uploaded': '2023-10-18 11:35:06.903148', 'head': 'O'},\n",
       "   {'date_uploaded': '2023-10-18 11:35:06.903148', 'head': '\\n'},\n",
       "   {'date_uploaded': '2023-10-18 11:35:06.903148', 'head': '3'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['R1: Installing Kubernetes Objective ● Define Kubernetes cluster configuration ● Understand infrastructure where kubernetes is installed Configuration There are many different cluster configurations that can be used when installing kubernetes. Some of the configurations are described below: 1. All in One single node installation: Both master and worker nodes components are installed and running on a single node. This is used during learning and testing, it is not advisable for production. 2. Single master and multi worker: Single master node managing multiple worker nodes. The single master node runs a multi-stacked etcd instance. 3. Single master with single node etcd and multi worker nodes: We have a single master node with an external etcd instance. This master node manages the multiple worker nodes. 4. Multi master and multi worker: This configuration is mainly used for high availability . Multi master nodes are configured with stacked etcd instances installed in each master node. It is recommended to install kubernetes on a multi host environment, with support for high availability control plane setups, and multiple worker nodes for client workload . Infrastructure We need to determine the infrastructure for installing kubernetes cluster. This is determined by the environment type e.g learning or production. We need to decide the following: 1. How we should setup kubernetes: On bare metal, public cloud, private or hybrid 2. OS type: Linux or Windows 3. Network solution Localhost Installation ● Minikube - single-node local kubernetes cluster, recommended for a learning environment deployed on a single host. ● Kind - multi-node kubernetes cluster deployed in docker containers, acting as kubernetes nodes, recommended for a learning environment. ● Docker Desktop - including a local kubernetes cluster for docker users. ● MicroK8s - local and cloud kubernetes cluster, from canonical. ● K3S - lightweight kubernetes cluster for local, cloud, edge, IoT deployments, from Rancher.',\n",
       "   \"Course: Introduction to Kubernetes L1:Introduction In this course, we will explore what kubernetes is; its architecture and building blocks, how it can be run on our local system or in the cloud, different ways we can configure and protect sensitive information, and how we can let external applications access our kubernetes application. We will also learn how to deploy and manage applications and resources with kubernetes. Here is a guide on how to create a GCP/Azure account Before diving into Kubernetes, we need to know some fundamental services that make working with Kubernetes easy and understandable. Containers Containers are an application-centric method used to deliver high-performing, scalable applications on any infrastructure of your choice. Containers provide a portable, isolated way of deploying microservices without disturbance from other microservices in our application. These containers install all the other dependencies needed by the microservice to function in a virtual environment. Containers are responsible for running container images, they do not run the microservices, rather, they couple the microservices and their dependencies. Container Images An image bundles the application with its runtime, libraries, and other dependencies. This serves as an isolated environment that runs the application. The runtime is the programming language that the application is built with or runs on. A container image represents binary data that encapsulates an application and all its software dependencies. Container images are executable software bundles that can run standalone and that make very well defined assumptions about their runtime environment. You typically create a container image of your application and push it to a registry before referring to it in a pod Microservices Microservices are simple, portable, and light applications written in major programming languages such as Rust, Go, Python, etc. Microservices have specific dependencies, libraries, and environmental requirements. Microservices must be deployed with their dependencies before they can successfully run.\\n\\nTools which group systems together to form clusters where containers' deployment and management is automated at scale are called a. DEployment b. Container c. Images d. Container orchestrators Question 4 What does a container image encapsulate? a. Represents a binary data that encapsulates an application and all its OS dependencies. b. Represents a binary data structure that encapsulates an application and all its binary dependencies c. Represents a binary data that encapsulates an application and all its Software dependencies. d. None of the above. Question 5 Simple, portable, and light applications written in major programming languages that also have specific dependencies, libraries, and environmental requirements are a. Microservices b. Containers c. Docker d. Images L2: Kubernetes Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Kubernetes can be referred to as k8s(pronounced Kate’s), 8 refers to the 8 characters between k and s. Kubernetes is an open source project started by Google and written in the Go programming language. The Kubernetes project was donated to the Cloud Native Computing foundation by Google, and new versions are released every 3 months. Features of Kubernetes Kubernetes offer a wide variety of features for orchestrating containers including: 1. Automatic bin packing: Containers are scheduled based on constraints and needed resources, this helps in maximizing utilization without sacrificing availability. 2. Self Healing: Kubernetes ensures a new pod is created to replace an unhealthy or a dead pod.\",\n",
       "   'On Premise Installation For on premises, kubernetes can be installed on VMs or bare metal On-premise VMs: K8s can be installed on VMs created via Vagrant, VMware vSphere, KVM, or a configuration tool in conjunction with a hypervisor. Bare Metal: K8s can be installed on premise bare metal hosted on top different os. Cloud Installation Kubernetes can be installed on almost any cloud production environment. A few of them are listed below. Hosted SolutionsWith Hosted Solutions, any given software is completely managed by the provider, while the user pays hosting and management charges. Popular vendors providing hosted solutions for Kubernetes are (listed in alphabetical order): Alibaba Cloud Container Service for Kubernetes (ACK) Amazon Elastic Kubernetes Service (EKS) Azure Kubernetes Service (AKS) DigitalOcean Kubernetes Google Kubernetes Engine (GKE) IBM Cloud Kubernetes Service Oracle Cloud Container Engine for Kubernetes (OKE) Turnkey Cloud Solutions Below are only a few of the Turnkey Cloud Solutions (listed in alphabetical order), to install kubernetes on an underlying IaaS platform such as: Alibaba Cloud Amazon AWS (AWS EC2) Google Compute Engine (GCE) IBM Cloud Private Microsoft Azure (AKS). Turnkey On-Premise SolutionsThe On-Premise solutions install kubernetes on secure internal private clouds: GKE On-Prem part of Google Cloud Anthos IBM Private Cloud',\n",
       "   '\\nWe can stop minikube with Minikube stop Accessing Kubernetes Kubernetes cluster can be accessed using the following methods 1. Command Line Interface (CLI) 2. Web-based User Interface. 3. APIs from CLI or programmatically . CLI Kubectl is the only kubernetes command line interface client that is used to manage cluster resources and applications. It is easily integrated into other systems, can be used in a script or for automation. Kubectl has to be configured with the credentials of the kubernetes cluster before it can be used. Web-based UI Kubernetes has a web based UI that is used to manage, monitor, and containerize applications and resources in a cluster. API server This is the main component of the kubernetes control plane, its responsibility is exposing the kubernetes APIs. These APIs allow users to interact with clusters directly. The CLI tools and Dashboard UI can be used to access the API server and perform various operations on the nodes.',\n",
       "   '3. Horizontal scaling: Applications are scaled manually or automatically based on CPU or custom metrics utilization. 4. Service discovery and load balancing: Containers receive their own IP addresses from Kubernetes, while it assigns a single Domain Name System (DNS) name to a set of containers to aid in load-balancing requests across the containers of the set. 5. Automated rollouts and rollbacks: Seamless roll out and roll back application updates and configuration changes, constantly monitoring application health to prevent downtimes. 6. Storage orchestration: Mounts software-defined storage solutions to containers from local storage, external cloud providers, distributed storage, and network storage systems. 7. Batch execution: Kubernetes supports batch execution, long-running jobs, and replacement for failed containers. Architecture Kubernetes contains the following components at a high level 1. One or more master nodes. This is part of the control plane. 2. One or more worker nodes Master node: The master node is responsible for providing a running environment for the control plane. This control plane manages the affairs of a Kubernetes cluster. The control plane also contains other agents with separate responsibilities, and they help with cluster management too. Communication between users and the k8s cluster is done through the following means; the application programming interface (API), command-line interface (CLI), and Web user interface (Web UI). All these act as a channel for the user to send the requests to the control plane managing the k8s clusters. The control plane should be running at all times because it is an important service. Also, a disruption might result in downtime which may affect businesses as a result of data loss.']]}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_from_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6c32aa8ec11f8121e8d7c496e6e09dd9',\n",
       " '1cac21d57ab91cf61ce0af52bfdc8db6',\n",
       " 'd14aac94004bca3680e49a6169ff89cc',\n",
       " 'ccfddfd8ab381ea3711c36a8c411d937',\n",
       " 'c515c071ec94d678b0aeff2485a9c074']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_from_chroma['ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 19551b58f60e0ded85d29c635ca4e5f1, 0.14350228015213295, R1: Installing Kubernetes Objective ● Define Kubernetes cluster configuration ● Understand infrastructure where kubernetes is installed Configuration There are many different cluster configurations that can be used when installing kubernetes. Some of the configurations are described below: 1. All in One single node installation: Both master and worker nodes components are installed and running on a single node. This is used during learning and testing, it is not advisable for production. 2. Single master and multi worker: Single master node managing multiple worker nodes. The single master node runs a multi-stacked etcd instance. 3. Single master with single node etcd and multi worker nodes: We have a single master node with an external etcd instance. This master node manages the multiple worker nodes. 4. Multi master and multi worker: This configuration is mainly used for high availability . Multi master nodes are configured with stacked etcd instances installed in each master node. It is recommended to install kubernetes on a multi host environment, with support for high availability control plane setups, and multiple worker nodes for client workload . Infrastructure We need to determine the infrastructure for installing kubernetes cluster. This is determined by the environment type e.g learning or production. We need to decide the following: 1. How we should setup kubernetes: On bare metal, public cloud, private or hybrid 2. OS type: Linux or Windows 3. Network solution Localhost Installation ● Minikube - single-node local kubernetes cluster, recommended for a learning environment deployed on a single host. ● Kind - multi-node kubernetes cluster deployed in docker containers, acting as kubernetes nodes, recommended for a learning environment. ● Docker Desktop - including a local kubernetes cluster for docker users. ● MicroK8s - local and cloud kubernetes cluster, from canonical. ● K3S - lightweight kubernetes cluster for local, cloud, edge, IoT deployments, from Rancher.\n",
      "\n",
      "id: e6f6650f9b61b98de63bd87f092d153d, 0.15193963205075378, Course: Introduction to Kubernetes L1:Introduction In this course, we will explore what kubernetes is; its architecture and building blocks, how it can be run on our local system or in the cloud, different ways we can configure and protect sensitive information, and how we can let external applications access our kubernetes application. We will also learn how to deploy and manage applications and resources with kubernetes. Here is a guide on how to create a GCP/Azure account Before diving into Kubernetes, we need to know some fundamental services that make working with Kubernetes easy and understandable. Containers Containers are an application-centric method used to deliver high-performing, scalable applications on any infrastructure of your choice. Containers provide a portable, isolated way of deploying microservices without disturbance from other microservices in our application. These containers install all the other dependencies needed by the microservice to function in a virtual environment. Containers are responsible for running container images, they do not run the microservices, rather, they couple the microservices and their dependencies. Container Images An image bundles the application with its runtime, libraries, and other dependencies. This serves as an isolated environment that runs the application. The runtime is the programming language that the application is built with or runs on. A container image represents binary data that encapsulates an application and all its software dependencies. Container images are executable software bundles that can run standalone and that make very well defined assumptions about their runtime environment. You typically create a container image of your application and push it to a registry before referring to it in a pod Microservices Microservices are simple, portable, and light applications written in major programming languages such as Rust, Go, Python, etc. Microservices have specific dependencies, libraries, and environmental requirements. Microservices must be deployed with their dependencies before they can successfully run.\n",
      "\n",
      "Tools which group systems together to form clusters where containers' deployment and management is automated at scale are called a. DEployment b. Container c. Images d. Container orchestrators Question 4 What does a container image encapsulate? a. Represents a binary data that encapsulates an application and all its OS dependencies. b. Represents a binary data structure that encapsulates an application and all its binary dependencies c. Represents a binary data that encapsulates an application and all its Software dependencies. d. None of the above. Question 5 Simple, portable, and light applications written in major programming languages that also have specific dependencies, libraries, and environmental requirements are a. Microservices b. Containers c. Docker d. Images L2: Kubernetes Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Kubernetes can be referred to as k8s(pronounced Kate’s), 8 refers to the 8 characters between k and s. Kubernetes is an open source project started by Google and written in the Go programming language. The Kubernetes project was donated to the Cloud Native Computing foundation by Google, and new versions are released every 3 months. Features of Kubernetes Kubernetes offer a wide variety of features for orchestrating containers including: 1. Automatic bin packing: Containers are scheduled based on constraints and needed resources, this helps in maximizing utilization without sacrificing availability. 2. Self Healing: Kubernetes ensures a new pod is created to replace an unhealthy or a dead pod.\n",
      "\n",
      "id: b46e395aaf05e8f1e8e74bd574a01f97, 0.152135480726164, On Premise Installation For on premises, kubernetes can be installed on VMs or bare metal On-premise VMs: K8s can be installed on VMs created via Vagrant, VMware vSphere, KVM, or a configuration tool in conjunction with a hypervisor. Bare Metal: K8s can be installed on premise bare metal hosted on top different os. Cloud Installation Kubernetes can be installed on almost any cloud production environment. A few of them are listed below. Hosted SolutionsWith Hosted Solutions, any given software is completely managed by the provider, while the user pays hosting and management charges. Popular vendors providing hosted solutions for Kubernetes are (listed in alphabetical order): Alibaba Cloud Container Service for Kubernetes (ACK) Amazon Elastic Kubernetes Service (EKS) Azure Kubernetes Service (AKS) DigitalOcean Kubernetes Google Kubernetes Engine (GKE) IBM Cloud Kubernetes Service Oracle Cloud Container Engine for Kubernetes (OKE) Turnkey Cloud Solutions Below are only a few of the Turnkey Cloud Solutions (listed in alphabetical order), to install kubernetes on an underlying IaaS platform such as: Alibaba Cloud Amazon AWS (AWS EC2) Google Compute Engine (GCE) IBM Cloud Private Microsoft Azure (AKS). Turnkey On-Premise SolutionsThe On-Premise solutions install kubernetes on secure internal private clouds: GKE On-Prem part of Google Cloud Anthos IBM Private Cloud\n",
      "\n",
      "id: 2ac07e37162a9b0a64cdf6b4a5ae9927, 0.15850031407387533, \n",
      "We can stop minikube with Minikube stop Accessing Kubernetes Kubernetes cluster can be accessed using the following methods 1. Command Line Interface (CLI) 2. Web-based User Interface. 3. APIs from CLI or programmatically . CLI Kubectl is the only kubernetes command line interface client that is used to manage cluster resources and applications. It is easily integrated into other systems, can be used in a script or for automation. Kubectl has to be configured with the credentials of the kubernetes cluster before it can be used. Web-based UI Kubernetes has a web based UI that is used to manage, monitor, and containerize applications and resources in a cluster. API server This is the main component of the kubernetes control plane, its responsibility is exposing the kubernetes APIs. These APIs allow users to interact with clusters directly. The CLI tools and Dashboard UI can be used to access the API server and perform various operations on the nodes.\n",
      "\n",
      "id: fc8a457e7b543faad654c3d50ba94b24, 0.16811634666797715, 3. Horizontal scaling: Applications are scaled manually or automatically based on CPU or custom metrics utilization. 4. Service discovery and load balancing: Containers receive their own IP addresses from Kubernetes, while it assigns a single Domain Name System (DNS) name to a set of containers to aid in load-balancing requests across the containers of the set. 5. Automated rollouts and rollbacks: Seamless roll out and roll back application updates and configuration changes, constantly monitoring application health to prevent downtimes. 6. Storage orchestration: Mounts software-defined storage solutions to containers from local storage, external cloud providers, distributed storage, and network storage systems. 7. Batch execution: Kubernetes supports batch execution, long-running jobs, and replacement for failed containers. Architecture Kubernetes contains the following components at a high level 1. One or more master nodes. This is part of the control plane. 2. One or more worker nodes Master node: The master node is responsible for providing a running environment for the control plane. This control plane manages the affairs of a Kubernetes cluster. The control plane also contains other agents with separate responsibilities, and they help with cluster management too. Communication between users and the k8s cluster is done through the following means; the application programming interface (API), command-line interface (CLI), and Web user interface (Web UI). All these act as a channel for the user to send the requests to the control plane managing the k8s clusters. The control plane should be running at all times because it is an important service. Also, a disruption might result in downtime which may affect businesses as a result of data loss.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(results_from_chroma['documents'][0]):\n",
    "    print(f\"id: {results_from_chroma['ids'][0][i]}, {results_from_chroma['distances'][0][i]}, {doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Crossencoder to re-rank search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_chroma(query, top_k=3, re_rank=False, verbose=True):\n",
    "    results_from_chroma = query_from_chroma(query, collection, top_k=5)\n",
    "    if not results_from_chroma:\n",
    "        return []\n",
    "    if verbose:\n",
    "        print(\"Query: \", query)\n",
    "    final_results = []\n",
    "\n",
    "    if re_rank:\n",
    "        if verbose:\n",
    "            print('Document ID (Hash)\\t\\tRetrieval Score\\tCE Score\\tText')\n",
    "\n",
    "        sentence_combinations = [query, results_from_chroma['document']]\n",
    "\n",
    "        # Compute the similarity scores for these combinations\n",
    "        similarity_scores = cross_encoder.predict(sentence_combinations)\n",
    "\n",
    "        # Sort the scores in decreasing order\n",
    "        sim_scores_argsort = reversed(np.argsort(similarity_scores))\n",
    "\n",
    "        # Print the scores\n",
    "        for idx in sim_scores_argsort:\n",
    "            result_from_chroma = results_from_chroma['document'][idx]\n",
    "            final_results.append(result_from_chroma)\n",
    "            if verbose:\n",
    "                print(f\"{results_from_chroma['ids'][idx]}\\t{result_from_chroma['distances']['idx']:.2f}\\t{similarity_scores[idx]:.2f}\\t{results_from_chroma['documents'][idx][:50]}\")\n",
    "        return final_results\n",
    "\n",
    "    if verbose:\n",
    "        print('Document ID (Hash)\\t\\tRetrieval Score\\tText')\n",
    "\n",
    "    for i, doc in enumerate(results_from_chroma['documents'][0]):\n",
    "        print(f\"id: {results_from_chroma['ids'][0][i]}, {results_from_chroma['distances'][0][i]}, {doc}\\n\")\n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
